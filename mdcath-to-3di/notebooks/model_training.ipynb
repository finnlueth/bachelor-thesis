{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS Availible:\t True\n",
      "Using device:\t cpu\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from _header_model import *\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "print(\"MPS Availible:\\t\", torch.backends.mps.is_available())\n",
    "print(f\"Using device:\\t {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### DataLoader\n",
    "Create HF Dataset\n",
    "\n",
    "```py\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['input_ids', 'attention_mask', 'labels', 'cath_id','temperature', 'replica'],\n",
    "        num_rows: n\n",
    "    })\n",
    "    valid: Dataset({\n",
    "        features: ['input_ids', 'attention_mask', 'labels', 'cath_id','temperature', 'replica'],\n",
    "        num_rows: n\n",
    "    })\n",
    "})\n",
    "```\n",
    "\n",
    "```rust\n",
    "input_ids: Amino Acid Sequence\n",
    "attention_mask: Padding Mask\n",
    "cath_id: cath identifier i.e. 1a0rP01\n",
    "replica: replica numbner in {0, 1, 2, 3, 4}\n",
    "temperature: temperature of trajectory in {320, 348, 379, 413, 450}\n",
    "sequence: original sequence\n",
    "pssm: PSSM as numpy array dim(20, L)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "414 : 7 5 15 9 15 23 5 18 3 8 7 5 6 12 19 17 10 7 5 3 7 11 14 7 5 3 18 12 11 13 3 5 9 11 5 5 3 12 5 8 4 5 17 16 3 10 11 18 6 9 19 17 4 9 23 14 16 11 4 10 17 5 3 11 11 8 15 14 6 19 6 3 10 5 16 11 7 18 17 10 21 11 3 7 11 7 10 4 17 6 8 16 3 15 6 9 4 5 17 4 13 11 15 3 5 13 15 14 5 7 11 4 21 3 5 14 8 15 10 8 10 17 15 10 12 23 21 12 10 7 10 6 6 15 4 3 5 11 5 5 5 12 18 10 6 14 21 17 10 5 4 8 7 17 15 7 4 18 5 8 17 15 5 10 12 10 10 7 7 17 7 6 16 17 18 12 4 11 19 17 23 15 3 5 13 4 16 19 19 6 7 5 4 8 3 14 10 17 10 9 8 14 10 7 17 5 17 4 3 14 5 10 3 3 17 11 5 6 23 3 4 4 5 4 23 17 10 7 15 18 5 4 8 10 5 7 7 14 11 3 4 4 18 5 23 5 4 5 3 9 6 14 5 12 5 7 10 5 3 4 8 13 5 3 10 11 21 8 12 3 7 18 5 11 11 13 4 7 9 17 21 7 6 3 13 3 19 4 3 16 8 7 14 10 8 18 3 10 5 10 7 18 16 21 3 11 15 17 4 8 4 12 16 3 12 17 16 17 15 3 4 3 18 9 5 7 18 16 18 19 10 4 14 13 9 5 18 17 10 8 16 3 6 17 5 7 15 18 14 4 11 15 3 13 11 15 14 6 5 7 12 5 10 15 15 7 8 13 9 12 8 15 18 11 7 21 19 10 21 7 14 14 4 17 17 18 3 7 10 10 3 4 5 7 10 5 15 17 7 5 5 9 21 7 15 5 6 16 19 9 11 21 15 1\n",
      "414 : 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-inf</td>\n",
       "      <td>1.632268</td>\n",
       "      <td>1.847997</td>\n",
       "      <td>0.201634</td>\n",
       "      <td>0.887525</td>\n",
       "      <td>2.711495</td>\n",
       "      <td>2.336283</td>\n",
       "      <td>2.419539</td>\n",
       "      <td>1.807355</td>\n",
       "      <td>0.378512</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.321928</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-2.736966</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-1.152003</td>\n",
       "      <td>-1.736966</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.736966</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-2.736966</td>\n",
       "      <td>-2.321928</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137504</td>\n",
       "      <td>-2.736966</td>\n",
       "      <td>-2.736966</td>\n",
       "      <td>-2.736966</td>\n",
       "      <td>-2.321928</td>\n",
       "      <td>-3.321928</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.459432</td>\n",
       "      <td>1.104337</td>\n",
       "      <td>2.510962</td>\n",
       "      <td>2.364572</td>\n",
       "      <td>1.485427</td>\n",
       "      <td>1.432959</td>\n",
       "      <td>1.405992</td>\n",
       "      <td>2.307429</td>\n",
       "      <td>1.722466</td>\n",
       "      <td>1.536053</td>\n",
       "      <td>...</td>\n",
       "      <td>2.419539</td>\n",
       "      <td>1.608809</td>\n",
       "      <td>1.906891</td>\n",
       "      <td>2.655352</td>\n",
       "      <td>3.008989</td>\n",
       "      <td>3.061776</td>\n",
       "      <td>2.847997</td>\n",
       "      <td>3.300124</td>\n",
       "      <td>2.217231</td>\n",
       "      <td>4.459432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-2.736966</td>\n",
       "      <td>-2.736966</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>-0.074001</td>\n",
       "      <td>-0.862496</td>\n",
       "      <td>-1.514573</td>\n",
       "      <td>0.432959</td>\n",
       "      <td>-3.321928</td>\n",
       "      <td>...</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-inf</td>\n",
       "      <td>1.632268</td>\n",
       "      <td>0.536053</td>\n",
       "      <td>1.104337</td>\n",
       "      <td>2.292782</td>\n",
       "      <td>2.392317</td>\n",
       "      <td>2.485427</td>\n",
       "      <td>1.560715</td>\n",
       "      <td>2.035624</td>\n",
       "      <td>1.169925</td>\n",
       "      <td>...</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-1.514573</td>\n",
       "      <td>-inf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722466</td>\n",
       "      <td>0.847997</td>\n",
       "      <td>1.432959</td>\n",
       "      <td>1.292782</td>\n",
       "      <td>0.201634</td>\n",
       "      <td>1.378512</td>\n",
       "      <td>-1.152003</td>\n",
       "      <td>1.292782</td>\n",
       "      <td>1.847997</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-3.321928</td>\n",
       "      <td>-3.321928</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963474</td>\n",
       "      <td>-0.234465</td>\n",
       "      <td>0.765535</td>\n",
       "      <td>0.765535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584963</td>\n",
       "      <td>-0.621488</td>\n",
       "      <td>1.232661</td>\n",
       "      <td>0.263034</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074001</td>\n",
       "      <td>0.201634</td>\n",
       "      <td>0.765535</td>\n",
       "      <td>1.632268</td>\n",
       "      <td>-0.152003</td>\n",
       "      <td>-1.514573</td>\n",
       "      <td>0.925999</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-1.514573</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.432959</td>\n",
       "      <td>-1.736966</td>\n",
       "      <td>-0.415037</td>\n",
       "      <td>1.632268</td>\n",
       "      <td>0.137504</td>\n",
       "      <td>1.722466</td>\n",
       "      <td>...</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-inf</td>\n",
       "      <td>1.925999</td>\n",
       "      <td>1.292782</td>\n",
       "      <td>2.419539</td>\n",
       "      <td>0.321928</td>\n",
       "      <td>-0.415037</td>\n",
       "      <td>-0.321928</td>\n",
       "      <td>-0.736966</td>\n",
       "      <td>0.137504</td>\n",
       "      <td>1.722466</td>\n",
       "      <td>...</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.152003</td>\n",
       "      <td>1.459432</td>\n",
       "      <td>1.944858</td>\n",
       "      <td>-inf</td>\n",
       "      <td>1.584963</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-1.736966</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-1.514573</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137504</td>\n",
       "      <td>0.321928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.514573</td>\n",
       "      <td>-0.862496</td>\n",
       "      <td>-1.321928</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.321928</td>\n",
       "      <td>-0.074001</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-inf</td>\n",
       "      <td>1.963474</td>\n",
       "      <td>1.584963</td>\n",
       "      <td>1.292782</td>\n",
       "      <td>0.321928</td>\n",
       "      <td>0.963474</td>\n",
       "      <td>-1.514573</td>\n",
       "      <td>-0.514573</td>\n",
       "      <td>0.807355</td>\n",
       "      <td>1.432959</td>\n",
       "      <td>...</td>\n",
       "      <td>1.584963</td>\n",
       "      <td>1.263034</td>\n",
       "      <td>1.321928</td>\n",
       "      <td>0.722466</td>\n",
       "      <td>0.925999</td>\n",
       "      <td>1.722466</td>\n",
       "      <td>2.472488</td>\n",
       "      <td>1.510962</td>\n",
       "      <td>1.321928</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-inf</td>\n",
       "      <td>2.017922</td>\n",
       "      <td>1.744161</td>\n",
       "      <td>2.277985</td>\n",
       "      <td>0.485427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.035624</td>\n",
       "      <td>0.485427</td>\n",
       "      <td>1.263034</td>\n",
       "      <td>1.807355</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.321928</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-2.321928</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>-2.736966</td>\n",
       "      <td>-inf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925999</td>\n",
       "      <td>1.906891</td>\n",
       "      <td>1.744161</td>\n",
       "      <td>-0.074001</td>\n",
       "      <td>1.070389</td>\n",
       "      <td>-0.152003</td>\n",
       "      <td>-1.514573</td>\n",
       "      <td>0.584963</td>\n",
       "      <td>1.807355</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765535</td>\n",
       "      <td>2.201634</td>\n",
       "      <td>1.378512</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.415037</td>\n",
       "      <td>-2.321928</td>\n",
       "      <td>-1.321928</td>\n",
       "      <td>-0.152003</td>\n",
       "      <td>1.378512</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>-inf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263034</td>\n",
       "      <td>1.201634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.765535</td>\n",
       "      <td>-0.415037</td>\n",
       "      <td>1.847997</td>\n",
       "      <td>-1.321928</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>-0.074001</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-inf</td>\n",
       "      <td>0.263034</td>\n",
       "      <td>0.321928</td>\n",
       "      <td>-1.321928</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.415037</td>\n",
       "      <td>-2.736966</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.862496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432959</td>\n",
       "      <td>-0.514573</td>\n",
       "      <td>-1.321928</td>\n",
       "      <td>-0.862496</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.736966</td>\n",
       "      <td>0.536053</td>\n",
       "      <td>0.678072</td>\n",
       "      <td>-0.074001</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-3.321928</td>\n",
       "      <td>2.744161</td>\n",
       "      <td>-0.415037</td>\n",
       "      <td>1.744161</td>\n",
       "      <td>1.169925</td>\n",
       "      <td>1.232661</td>\n",
       "      <td>0.847997</td>\n",
       "      <td>...</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>-3.321928</td>\n",
       "      <td>-3.321928</td>\n",
       "      <td>-2.736966</td>\n",
       "      <td>-4.321928</td>\n",
       "      <td>-1.152003</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 413 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0       -inf  1.632268  1.847997  0.201634  0.887525  2.711495  2.336283   \n",
       "1       -inf -1.152003 -1.736966 -2.000000 -2.736966 -4.321928      -inf   \n",
       "2   4.459432  1.104337  2.510962  2.364572  1.485427  1.432959  1.405992   \n",
       "3       -inf      -inf -2.736966 -2.736966 -4.321928 -0.074001 -0.862496   \n",
       "4       -inf  1.632268  0.536053  1.104337  2.292782  2.392317  2.485427   \n",
       "5       -inf      -inf      -inf      -inf      -inf      -inf -4.321928   \n",
       "6       -inf      -inf      -inf      -inf -4.321928      -inf -3.321928   \n",
       "7       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
       "8       -inf      -inf -2.000000      -inf  0.432959 -1.736966 -0.415037   \n",
       "9       -inf  1.925999  1.292782  2.419539  0.321928 -0.415037 -0.321928   \n",
       "10      -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
       "11      -inf      -inf      -inf      -inf      -inf      -inf -1.514573   \n",
       "12      -inf  1.963474  1.584963  1.292782  0.321928  0.963474 -1.514573   \n",
       "13      -inf  2.017922  1.744161  2.277985  0.485427  1.000000  1.035624   \n",
       "14      -inf      -inf      -inf      -inf      -inf      -inf -2.321928   \n",
       "15      -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
       "16      -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
       "17      -inf  0.263034  0.321928 -1.321928 -2.000000 -0.415037 -2.736966   \n",
       "18      -inf -2.000000 -1.000000 -3.321928  2.744161 -0.415037  1.744161   \n",
       "19      -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
       "\n",
       "         7         8         9    ...       403       404       405       406  \\\n",
       "0   2.419539  1.807355  0.378512  ... -3.321928      -inf      -inf      -inf   \n",
       "1  -2.736966 -2.321928 -2.000000  ...  0.137504 -2.736966 -2.736966 -2.736966   \n",
       "2   2.307429  1.722466  1.536053  ...  2.419539  1.608809  1.906891  2.655352   \n",
       "3  -1.514573  0.432959 -3.321928  ...      -inf      -inf      -inf      -inf   \n",
       "4   1.560715  2.035624  1.169925  ...      -inf      -inf      -inf      -inf   \n",
       "5       -inf -1.514573      -inf  ...  0.722466  0.847997  1.432959  1.292782   \n",
       "6  -3.321928      -inf      -inf  ...  0.963474 -0.234465  0.765535  0.765535   \n",
       "7       -inf      -inf      -inf  ... -0.074001  0.201634  0.765535  1.632268   \n",
       "8   1.632268  0.137504  1.722466  ...      -inf      -inf      -inf      -inf   \n",
       "9  -0.736966  0.137504  1.722466  ...      -inf      -inf      -inf      -inf   \n",
       "10      -inf      -inf      -inf  ... -1.000000      -inf -0.152003  1.459432   \n",
       "11      -inf -4.321928 -4.321928  ...  0.137504  0.321928  0.000000 -1.514573   \n",
       "12 -0.514573  0.807355  1.432959  ...  1.584963  1.263034  1.321928  0.722466   \n",
       "13  0.485427  1.263034  1.807355  ... -3.321928      -inf      -inf -4.321928   \n",
       "14 -4.321928 -2.736966      -inf  ...  0.925999  1.906891  1.744161 -0.074001   \n",
       "15      -inf      -inf      -inf  ...  0.765535  2.201634  1.378512 -2.000000   \n",
       "16      -inf -4.321928      -inf  ...  0.263034  1.201634  0.000000  0.765535   \n",
       "17 -4.321928 -2.000000 -0.862496  ...  0.432959 -0.514573 -1.321928 -0.862496   \n",
       "18  1.169925  1.232661  0.847997  ...      -inf      -inf      -inf      -inf   \n",
       "19      -inf      -inf      -inf  ... -4.321928      -inf -2.000000 -4.321928   \n",
       "\n",
       "         407       408       409       410       411       412  \n",
       "0  -2.736966      -inf      -inf      -inf      -inf      -inf  \n",
       "1  -2.321928 -3.321928      -inf      -inf      -inf      -inf  \n",
       "2   3.008989  3.061776  2.847997  3.300124  2.217231  4.459432  \n",
       "3       -inf      -inf      -inf      -inf      -inf      -inf  \n",
       "4       -inf      -inf      -inf      -inf      -inf      -inf  \n",
       "5   0.201634  1.378512 -1.152003  1.292782  1.847997      -inf  \n",
       "6   0.000000  0.584963 -0.621488  1.232661  0.263034      -inf  \n",
       "7  -0.152003 -1.514573  0.925999      -inf -1.514573      -inf  \n",
       "8       -inf      -inf      -inf      -inf      -inf      -inf  \n",
       "9       -inf      -inf      -inf      -inf      -inf      -inf  \n",
       "10  1.944858      -inf  1.584963      -inf -1.736966      -inf  \n",
       "11 -0.862496 -1.321928 -1.000000 -1.321928 -0.074001      -inf  \n",
       "12  0.925999  1.722466  2.472488  1.510962  1.321928      -inf  \n",
       "13 -4.321928      -inf      -inf      -inf      -inf      -inf  \n",
       "14  1.070389 -0.152003 -1.514573  0.584963  1.807355      -inf  \n",
       "15 -0.415037 -2.321928 -1.321928 -0.152003  1.378512      -inf  \n",
       "16 -0.415037  1.847997 -1.321928 -4.321928 -0.074001      -inf  \n",
       "17 -1.000000 -0.736966  0.536053  0.678072 -0.074001      -inf  \n",
       "18      -inf      -inf      -inf      -inf      -inf      -inf  \n",
       "19 -3.321928 -3.321928 -2.736966 -4.321928 -1.152003      -inf  \n",
       "\n",
       "[20 rows x 413 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=BASE_MODEL,\n",
    "    do_lower_case=False,\n",
    "    use_fast=True,\n",
    "    legacy=False,\n",
    ")\n",
    "\n",
    "dict_pssm = np.load(os.path.join(FILE_PATHS[\"pssm\"], \"dict_pssm.npy\"), allow_pickle=True).item()\n",
    "\n",
    "\n",
    "def pssm_to_hf_dataset(dict_pssm: dict, tokenizer: T5Tokenizer) -> Dataset:\n",
    "    # todo add train/test split\n",
    "    ds_dict = {\"cath_id\": [], \"temperature\": [], \"replica\": [], \"sequence\": [], \"sequence_processed\": [], \"pssm\": []}\n",
    "\n",
    "    for key, value in dict_pssm.items():\n",
    "        parts = key.split(\"|\")\n",
    "        ds_dict[\"cath_id\"].append(parts[0])\n",
    "        ds_dict[\"temperature\"].append(int(parts[1]))\n",
    "        ds_dict[\"replica\"].append(parts[2])\n",
    "        ds_dict[\"sequence\"].append(parts[3])\n",
    "        ds_dict[\"sequence_processed\"].append([\" \".join(x) for x in parts[3]])\n",
    "        ds_dict[\"pssm\"].append(value.tolist())\n",
    "\n",
    "    tokenized_sequences = tokenizer(\n",
    "        text=[\" \".join(x) for x in ds_dict[\"sequence\"]],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    ds = Dataset.from_dict(tokenized_sequences)\n",
    "    for key, value in ds_dict.items():\n",
    "        ds = ds.add_column(key, value)\n",
    "    \n",
    "    # ds = ds.map(lambda examples: {'pssm': [torch.tensor(pssm) for pssm in examples['pssm']]}, batched=True)\n",
    "\n",
    "    return DatasetDict({\"train\": ds, \"test\": ds})\n",
    "\n",
    "\n",
    "ds = pssm_to_hf_dataset(dict_pssm=dict_pssm, tokenizer=t5_tokenizer)\n",
    "ds = ds.remove_columns([\"cath_id\", \"replica\", \"sequence\", 'sequence_processed', \"temperature\"])\n",
    "ds = ds.rename_column(\"pssm\", \"labels\")\n",
    "# ds = ds.remove_columns(\"labels\")\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].select([0, 49])\n",
    "ds[\"test\"] = ds[\"test\"].select([0, 49])\n",
    "\n",
    "print(ds)\n",
    "\n",
    "i = 1\n",
    "print(len(ds[\"train\"][\"attention_mask\"][i]), \":\", *ds[\"train\"][\"input_ids\"][i])\n",
    "print(len(ds[\"train\"][\"attention_mask\"][i]), ':', *ds[\"train\"][\"attention_mask\"][i])\n",
    "display(pd.DataFrame(ds[\"train\"][\"labels\"][i]))\n",
    "print(type(torch.tensor(ds[\"train\"][\"labels\"][i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Model Loading and LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5EncoderModelForPssmGeneration were not initialized from the model checkpoint at Rostlab/prot_t5_xl_uniref50 and are newly initialized: ['pssm_head.bn1.bias', 'pssm_head.bn1.num_batches_tracked', 'pssm_head.bn1.running_mean', 'pssm_head.bn1.running_var', 'pssm_head.bn1.weight', 'pssm_head.bn2.bias', 'pssm_head.bn2.num_batches_tracked', 'pssm_head.bn2.running_mean', 'pssm_head.bn2.running_var', 'pssm_head.bn2.weight', 'pssm_head.conv1.bias', 'pssm_head.conv1.weight', 'pssm_head.conv2.bias', 'pssm_head.conv2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,932,160 || all params: 1,216,534,272 || trainable%: 0.3232\n"
     ]
    }
   ],
   "source": [
    "t5_base_model, loading_info = T5EncoderModelForPssmGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=BASE_MODEL,\n",
    "    output_loading_info=True,\n",
    "    # device_map=device,\n",
    "    # load_in_8bit=False,\n",
    "    # custom_dropout_rate=0.1,\n",
    ")\n",
    "\n",
    "modules_to_save = [\"classifier\"]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q\", \"k\", \"v\", \"o\"],\n",
    "    bias=\"none\",\n",
    "    modules_to_save=loading_info['missing_keys'], # check if saving 'pssm_head' also works\n",
    ")\n",
    "\n",
    "t5_lora_model = peft.get_peft_model(t5_base_model, lora_config)\n",
    "t5_lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_keys': ['pssm_head.bn1.bias',\n",
       "  'pssm_head.bn1.num_batches_tracked',\n",
       "  'pssm_head.bn1.running_mean',\n",
       "  'pssm_head.bn1.running_var',\n",
       "  'pssm_head.bn1.weight',\n",
       "  'pssm_head.bn2.bias',\n",
       "  'pssm_head.bn2.num_batches_tracked',\n",
       "  'pssm_head.bn2.running_mean',\n",
       "  'pssm_head.bn2.running_var',\n",
       "  'pssm_head.bn2.weight',\n",
       "  'pssm_head.conv1.bias',\n",
       "  'pssm_head.conv1.weight',\n",
       "  'pssm_head.conv2.bias',\n",
       "  'pssm_head.conv2.weight'],\n",
       " 'unexpected_keys': ['lm_head.weight'],\n",
       " 'mismatched_keys': [],\n",
       " 'error_msgs': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loading_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finnlueth/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=t5_tokenizer,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=FILE_PATHS[\"models\"],\n",
    "    learning_rate=TRAINING_CONFIG[\"learning_rate\"],\n",
    "    per_device_train_batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=TRAINING_CONFIG[\"batch_size\"] * 2,\n",
    "    num_train_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
    "    logging_steps=TRAINING_CONFIG[\"logging_steps\"],\n",
    "    evaluation_strategy=\"steps\", # use eval_strategy\n",
    "    eval_steps=TRAINING_CONFIG[\"eval_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=TRAINING_CONFIG[\"save_steps\"],\n",
    "    remove_unused_columns=True,\n",
    "    # label_names=[\"labels\"],\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=t5_lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    # eval_dataset=dataset_signalp['valid'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*ds['train']['input_ids'], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in t5_base_model.named_parameters():\n",
    "#     print(name)\n",
    "\n",
    "# t5_base_model.encoder.block[0].layer[0].SelfAttention.q.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e61308f36b474a85bb12914ed5d703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finnlueth/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:762\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 762\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:724\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 413 at dim 2 (got 63)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m      6\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/transformers/trainer.py:2236\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2233\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2236\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/accelerate/data_loader.py:550\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[0;32m~/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3578\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3575\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3576\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:227\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    223\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 227\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/gits/bachelor-thesis/mdcath-to-3di/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:778\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    774\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    775\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    776\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    777\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    779\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    780\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    781\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    782\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    783\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent_class_name = T5EncoderModelForPssmGeneration.__bases__[0].__name__\n",
    "# parent_class_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
