{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS Availible:\t False\n",
      "Using device:\t cuda:0\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from _header_model import *\n",
    "\n",
    "# device = \"cpu\"\n",
    "\n",
    "print(\"MPS Availible:\\t\", torch.backends.mps.is_available())\n",
    "print(f\"Using device:\\t {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### DataLoader\n",
    "\n",
    "<!-- Create HF Dataset\n",
    "\n",
    "```py\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['input_ids', 'attention_mask', 'labels', 'cath_id','temperature', 'replica'],\n",
    "        num_rows: n\n",
    "    })\n",
    "    valid: Dataset({\n",
    "        features: ['input_ids', 'attention_mask', 'labels', 'cath_id','temperature', 'replica'],\n",
    "        num_rows: n\n",
    "    })\n",
    "})\n",
    "```\n",
    "\n",
    "```rust\n",
    "input_ids: Amino Acid Sequence\n",
    "attention_mask: Padding Mask\n",
    "cath_id: cath identifier i.e. 1a0rP01\n",
    "replica: replica numbner in {0, 1, 2, 3, 4}\n",
    "temperature: temperature of trajectory in {320, 348, 379, 413, 450}\n",
    "sequence: original sequence\n",
    "pssm: PSSM as numpy array dim(20, L)\n",
    "``` -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=BASE_MODEL,\n",
    "    do_lower_case=False,\n",
    "    use_fast=True,\n",
    "    legacy=False,\n",
    ")\n",
    "\n",
    "dict_pssm = np.load(os.path.join('../', FILE_PATHS[\"pssm\"], \"dict_pssm.npy\"), allow_pickle=True).item()\n",
    "\n",
    "\n",
    "def pssm_to_hf_dataset(dict_pssm: dict, tokenizer: T5Tokenizer) -> Dataset:\n",
    "    # todo add train/test split\n",
    "    ds_dict = {\"cath_id\": [], \"temperature\": [], \"replica\": [], \"sequence\": [], \"sequence_processed\": [], \"pssm\": []}\n",
    "\n",
    "    for key, value in dict_pssm.items():\n",
    "        parts = key.split(\"|\")\n",
    "        ds_dict[\"cath_id\"].append(parts[0])\n",
    "        ds_dict[\"temperature\"].append(int(parts[1]))\n",
    "        ds_dict[\"replica\"].append(parts[2])\n",
    "        ds_dict[\"sequence\"].append(parts[3])\n",
    "        ds_dict[\"sequence_processed\"].append(\" \".join(parts[3]))\n",
    "        ds_dict[\"pssm\"].append(value.tolist())\n",
    "\n",
    "    tokenized_sequences = tokenizer(\n",
    "        text=ds_dict[\"sequence_processed\"],\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        max_length=512,\n",
    "    )\n",
    "    ds = Dataset.from_dict(tokenized_sequences)\n",
    "    for key, value in ds_dict.items():\n",
    "        ds = ds.add_column(key, value)\n",
    "    \n",
    "    # ds = ds.map(lambda examples: {'pssm': [torch.tensor(pssm) for pssm in examples['pssm']]}, batched=True)\n",
    "\n",
    "    return DatasetDict({\"train\": ds, \"test\": ds})\n",
    "\n",
    "\n",
    "ds = pssm_to_hf_dataset(dict_pssm=dict_pssm, tokenizer=t5_tokenizer)\n",
    "ds = ds.remove_columns([\"cath_id\", \"replica\", \"sequence\", 'sequence_processed', \"temperature\"])\n",
    "ds = ds.rename_column(\"pssm\", \"labels\")\n",
    "# ds = ds.remove_columns(\"labels\")\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].select([0, 49])\n",
    "ds[\"test\"] = ds[\"test\"].select([0, 49])\n",
    "\n",
    "print(ds)\n",
    "\n",
    "# i = 0\n",
    "# print(len(ds[\"train\"][\"attention_mask\"][i]), \":\", *ds[\"train\"][\"input_ids\"][i])\n",
    "# print(len(ds[\"train\"][\"attention_mask\"][i]), ':', *ds[\"train\"][\"attention_mask\"][i])\n",
    "# display(pd.DataFrame(ds[\"train\"][\"labels\"][i]))\n",
    "# print(type(torch.tensor(ds[\"train\"][\"labels\"][i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Model Loading and LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base_model, loading_info = T5EncoderModelForPssmGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=BASE_MODEL,\n",
    "    output_loading_info=True,\n",
    "    # device_map=device,\n",
    "    # load_in_8bit=False,\n",
    "    # custom_dropout_rate=0.1,\n",
    ")\n",
    "\n",
    "modules_to_save = [\"classifier\"]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q\", \"k\", \"v\", \"o\"],\n",
    "    bias=\"none\",\n",
    "    modules_to_save=loading_info['missing_keys'], # check if saving 'pssm_head' also works\n",
    ")\n",
    "\n",
    "t5_lora_model = peft.get_peft_model(t5_base_model, lora_config)\n",
    "t5_lora_model.print_trainable_parameters()\n",
    "print(loading_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForT5Pssm(\n",
    "    tokenizer=t5_tokenizer,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=FILE_PATHS[\"models\"],\n",
    "    learning_rate=TRAINING_CONFIG[\"learning_rate\"],\n",
    "    per_device_train_batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=TRAINING_CONFIG[\"batch_size\"] * 2,\n",
    "    num_train_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
    "    logging_steps=TRAINING_CONFIG[\"logging_steps\"],\n",
    "    evaluation_strategy=\"steps\", # use eval_strategy\n",
    "    eval_steps=TRAINING_CONFIG[\"eval_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=TRAINING_CONFIG[\"save_steps\"],\n",
    "    remove_unused_columns=True,\n",
    "    # label_names=[\"labels\"],\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=t5_lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    # eval_dataset=dataset_signalp['valid'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*ds['train']['input_ids'], sep=\"\\n\")\n",
    "\n",
    "# for name, param in t5_base_model.named_parameters():\n",
    "#     print(name)\n",
    "\n",
    "# t5_base_model.encoder.block[0].layer[0].SelfAttention.q.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent_class_name = T5EncoderModelForPssmGeneration.__bases__[0].__name__\n",
    "# parent_class_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
