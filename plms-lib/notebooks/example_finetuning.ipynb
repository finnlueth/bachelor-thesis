{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import yaml\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import (\n",
    "    DataCollatorForTokenClassification,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "from plms import (\n",
    "    PLMConfig,\n",
    "    ProteinLanguageModelPredictor,\n",
    "    auto_model,\n",
    "    auto_tokenizer,\n",
    ")\n",
    "from plms.models.plm_token_classification import PLMConfigForTokenClassification, PLMForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_strings = [\n",
    "    (\"ACDEFGHIKLMNPQRSTVWYOOO\", \"aaaaaaaabbbbbbbbccccccc\"),\n",
    "    (\"ACDEFGHIKLMNPQRSTVWYACDEFGHIKLMNPQRSTVWYOOO\", \"aaaaaaaabbbbbbbbccccccaaaaaaaabbbbbbbbccccc\"),\n",
    "    (\"ACDEFGHIKLMNPQRSTVWYOOO\", \"aaaaaaaabbbbbbbbccccccc\"),\n",
    "    (\"ACDEFGHIKLMNPQRSTVWYACDEFGHIKLMNPQRSTVWYOOO\", \"aaaaaaaabbbbbbbbccccccaaaaaaaabbbbbbbbccccc\"),\n",
    "    (\"ACDEFGHIKLMNPQRSTVWYOOO\", \"aaaaaaaabbbbbbbbccccccc\"),\n",
    "    (\"ACDEFGHIKLMNPQRSTVWYOOO\", \"aaaaaaaabbbbbbbbccccccc\"),\n",
    "    (\"ACDEFGHIKLMNPQRSTVWYACDEFGHIKLMNPQRSTVWYOOO\", \"aaaaaaaabbbbbbbbccccccaaaaaaaabbbbbbbbccccc\"),\n",
    "    (\"ACDEFGHIKLMNPQRSTVWYACDEFGHIKLMNPQRSTVWYOOO\", \"aaaaaaaabbbbbbbbccccccaaaaaaaabbbbbbbbccccc\"),\n",
    "    (\"ACDEFGHIKLMNPQRSTVWYOOO\", \"aaaaaaaabbbbbbbbccccccc\"),\n",
    "    (\"ACDEFGHIKLMNPQRSTVWYACDEFGHIKLMNPQRSTVWYOOO\", \"aaaaaaaabbbbbbbbccccccaaaaaaaabbbbbbbbccccc\"),\n",
    "]\n",
    "\n",
    "config_yaml = \"\"\"\n",
    "metadata:\n",
    "  identifier: test123\n",
    "model:\n",
    "  encoder_name_or_path: Rostlab/ProstT5\n",
    "#   encoder_name_or_path: Rostlab/prot_t5_xl_uniref50\n",
    "  num_labels: 3\n",
    "  classifier_dropout: 0.1\n",
    "  hidden_size: 1024\n",
    "training_args:\n",
    "  output_dir: ./tmp/models/checkpoints\n",
    "  run_name: test\n",
    "#   report_to: None\n",
    "  learning_rate: 0.0001\n",
    "  per_device_train_batch_size: 6\n",
    "  per_device_eval_batch_size: 6\n",
    "  num_train_epochs: 100\n",
    "  logging_steps: 1\n",
    "  logging_strategy: steps\n",
    "  evaluation_strategy: steps\n",
    "  eval_steps: 1\n",
    "  eval_strategy: steps\n",
    "  eval_on_start: true\n",
    "  batch_eval_metrics: false\n",
    "  save_strategy: steps\n",
    "  save_steps: 300\n",
    "  save_total_limit: 5\n",
    "  remove_unused_columns: true\n",
    "  label_names: ['labels']\n",
    "  seed: 42\n",
    "  lr_scheduler_type: cosine\n",
    "  warmup_steps: 0\n",
    "lora:\n",
    "  inference_mode: false\n",
    "  r: 8\n",
    "  lora_alpha: 16\n",
    "  lora_dropout: 0.05\n",
    "  use_rslora: false\n",
    "  use_dora: false\n",
    "  target_modules: ['q', 'v']\n",
    "  bias: none\n",
    "data_collator:\n",
    "  padding: true\n",
    "  pad_to_multiple_of: 8\n",
    "extender:\n",
    "  name: Rostlab/ProstT5\n",
    "  use_extender: false\n",
    "\"\"\"\n",
    "\n",
    "LABEL_ENCODING = {\n",
    "    \"a\": 0,\n",
    "    \"b\": 1,\n",
    "    \"c\": 2,\n",
    "}\n",
    "\n",
    "config = yaml.safe_load(config_yaml)\n",
    "\n",
    "tokenizer = auto_tokenizer(config[\"model\"][\"encoder_name_or_path\"])\n",
    "\n",
    "dataset_dict = {\n",
    "    \"sequence\": [x[0] for x in dataset_strings],\n",
    "    \"labels\": [[LABEL_ENCODING[y] for y in x[1]] for x in dataset_strings],\n",
    "}\n",
    "dataset = datasets.Dataset.from_dict(dataset_dict)\n",
    "\n",
    "tokens = tokenizer.encode(dataset_dict[\"sequence\"])\n",
    "\n",
    "dataset = dataset.add_column(\"input_ids\", tokens[\"input_ids\"])\n",
    "dataset = dataset.add_column(\"attention_mask\", tokens[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = PLMConfigForTokenClassification(**config[\"model\"])\n",
    "model = PLMForTokenClassification(model_config)\n",
    "lora_config = LoraConfig(**config[\"lora\"], modules_to_save=model.get_modules_to_save())\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer.get_tokenizer(), **config[\"data_collator\"])\n",
    "\n",
    "training_args = TrainingArguments(**config[\"training_args\"])\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_dataset_dict = {\n",
    "    \"sequence\": [\"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\", \"ACDEFGHIKLMNPQRSTVWYOOO\"],\n",
    "    \"labels\": [\n",
    "        [0] * len(\"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGGxx\"),\n",
    "        [LABEL_ENCODING[y] for y in \"aaaaaaaaabbbbbbbbcccccccc\"],\n",
    "    ],\n",
    "}\n",
    "\n",
    "test_dataset = datasets.Dataset.from_dict(test_dataset_dict)\n",
    "\n",
    "test_tokens = tokenizer.encode(test_dataset_dict[\"sequence\"])\n",
    "\n",
    "test_dataset = test_dataset.add_column(\"input_ids\", test_tokens[\"input_ids\"])\n",
    "test_dataset = test_dataset.add_column(\"attention_mask\", test_tokens[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in test_dataset[1].items():\n",
    "    print(k, len(v), v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=torch.tensor(test_dataset[\"input_ids\"][index]).to(model.device).unsqueeze(0),\n",
    "        attention_mask=torch.tensor(test_dataset[\"attention_mask\"][index]).to(model.device).unsqueeze(0),\n",
    "        labels=torch.tensor(test_dataset[\"labels\"][index]).to(model.device).unsqueeze(0),\n",
    "    )\n",
    "\n",
    "LABEL_DECODING = {v: k for k, v in LABEL_ENCODING.items()}\n",
    "\n",
    "predictions = outputs.logits.argmax(dim=-1).tolist()\n",
    "print(*[LABEL_DECODING[pred] for pred in predictions[0]], sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_DECODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
