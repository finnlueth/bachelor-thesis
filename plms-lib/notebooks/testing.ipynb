{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/ProstT5\", do_lower_case=False)\n",
    "\n",
    "# Load the model\n",
    "# model = T5EncoderModel.from_pretrained(\"Rostlab/ProstT5\").to(device)\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "# model.full() if device=='cpu' else model.half()\n",
    "\n",
    "# prepare your protein sequences/structures as a list. Amino acid sequences are expected to be upper-case (\"PRTEINO\" below) while 3Di-sequences need to be lower-case (\"strctr\" below).\n",
    "sequence_examples = [\"PRTEINO\", \"strct\"]\n",
    "\n",
    "# replace all rare/ambiguous amino acids by X (3Di sequences does not have those) and introduce white-space between all sequences (AAs and 3Di)\n",
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "sequence_examples = [\"<AA2fold>\" + \" \" + s if s.isupper() else \"<fold2AA>\" + \" \" + s for s in sequence_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequence_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sequences and return pytorch tensors\n",
    "ids = tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, padding=True, return_tensors=\"pt\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode back to sequences\n",
    "decoded_sequences = tokenizer.batch_decode(ids[\"input_ids\"], skip_special_tokens=True)\n",
    "\n",
    "# remove spaces between amino acids\n",
    "decoded_sequences = [seq.replace(\" \", \"\") for seq in decoded_sequences]\n",
    "\n",
    "print(\"Decoded sequences:\", decoded_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5EncoderModel.from_pretrained(\"Rostlab/ProstT5\").to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/ProstT5\", do_lower_case=False)\n",
    "\n",
    "sequence_examples = [\"PRTEINO\", \"SEQWENCE\"]\n",
    "# this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids = tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, padding=\"longest\")\n",
    "input_ids = torch.tensor(ids[\"input_ids\"]).to(device)\n",
    "attention_mask = torch.tensor(ids[\"attention_mask\"]).to(device)\n",
    "\n",
    "# generate embeddings\n",
    "with torch.no_grad():\n",
    "    embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# extract embeddings for the first ([0,:]) sequence in the batch while removing padded & special tokens ([0,:7])\n",
    "emb_0 = embedding_repr.last_hidden_state[0, :7]  # shape (7 x 1024)\n",
    "print(f\"Shape of per-residue embedding of first sequences: {emb_0.shape}\")\n",
    "# do the same for the second ([1,:]) sequence in the batch while taking into account different sequence lengths ([1,:8])\n",
    "emb_1 = embedding_repr.last_hidden_state[1, :8]  # shape (8 x 1024)\n",
    "\n",
    "# if you want to derive a single representation (per-protein embedding) for the whole protein\n",
    "emb_0_per_protein = emb_0.mean(dim=0)  # shape (1024)\n",
    "\n",
    "print(f\"Shape of per-protein embedding of first sequences: {emb_0_per_protein.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
