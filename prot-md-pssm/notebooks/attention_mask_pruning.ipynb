{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "from src.model.utils.data_collator import DataCollatorForT5Pssm\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"../tmp/data/pssm/pssm_dataset_0_only/\")\n",
    "dataset = dataset.rename_column(\"pssm_features\", \"labels\")\n",
    "dataset = dataset.remove_columns([\"name\", \"sequence\", \"sequence_processed\"])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"Rostlab/prot_t5_xl_uniref50\",\n",
    "    do_lower_case=False,\n",
    "    use_fast=True,\n",
    "    legacy=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForT5Pssm(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [dataset[i] for i in range(100, 140)]\n",
    "batch = data_collator(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 256)\n",
    "pd.set_option(\"display.max_columns\", 256)\n",
    "# pd.DataFrame(batch[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(batch[\"attention_mask\"][0:100:5].tolist()))\n",
    "display(pd.DataFrame([x.replace(\"<\", \" <\").split(\" \") for x in tokenizer.batch_decode(batch[\"input_ids\"][0:100:5].tolist())]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = batch[\"attention_mask\"][0:100:5]\n",
    "\n",
    "print(attention_mask.device)\n",
    "attention_mask = attention_mask.to(\"cuda\")\n",
    "print(attention_mask.device)\n",
    "\n",
    "display(pd.DataFrame(attention_mask.tolist()).iloc[:, 70:])\n",
    "\n",
    "attention_mask = attention_mask.clone()  #!\n",
    "\n",
    "seq_lengths = attention_mask.sum(dim=1) - 1  #!\n",
    "\n",
    "print(\"seq_lengths:\", *seq_lengths.tolist())\n",
    "\n",
    "batch_indices = torch.arange(attention_mask.size(0), device=attention_mask.device)  #!\n",
    "print(\"batch_indices:\", *batch_indices.tolist())\n",
    "\n",
    "attention_mask[batch_indices, seq_lengths] = 0\n",
    "\n",
    "display(pd.DataFrame(attention_mask.tolist()).iloc[:, 70:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_embeddings = torch.randn(8, attention_mask.size(1), 1024, device=attention_mask.device)\n",
    "\n",
    "# Create a mask with shape [8, seq_len, 1024] by expanding attention_mask\n",
    "# masked_embeddings = random_embeddings * attention_mask[:, :, None].expand_as(random_embeddings)\n",
    "\n",
    "masked_embeddings = random_embeddings * attention_mask.unsqueeze(-1)\n",
    "\n",
    "display(pd.DataFrame(masked_embeddings.cpu()[-1]).iloc[70:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask.masked_fill(~attention_mask[:, None, :], float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", 32)\n",
    "\n",
    "random_embeddings = torch.randn(8, attention_mask.size(1), 1024, device=attention_mask.device)\n",
    "print(f\"Random embeddings shape: {random_embeddings.shape}\")\n",
    "\n",
    "display(pd.DataFrame(random_embeddings.cpu()[0]))\n",
    "\n",
    "masked_embeddings = random_embeddings * attention_mask[:, :, None]\n",
    "print(f\"Masked embeddings shape: {masked_embeddings.shape}\")\n",
    "display(pd.DataFrame(masked_embeddings.cpu()[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
