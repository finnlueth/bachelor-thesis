{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import random\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from Bio import BiopythonWarning\n",
    "from transformers import AutoModelForSeq2SeqLM, T5Tokenizer\n",
    "\n",
    "from src.model.configuration_md_pssm import MDPSSMConfig\n",
    "from src.model.modeling_md_pssm import T5EncoderModelForPssmGeneration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=BiopythonWarning)\n",
    "\n",
    "SCOP40_SEQUENCES_FILE = \"../tmp/data/scope/scope40_sequences.json\"\n",
    "MODEL_PATH = \"../tmp/models/adapters/prot-md-pssm-2025-03-05-17-43-47-full-dataset\"\n",
    "PROTEIN_ENCODER_NAME = \"Rostlab/prot_t5_xl_uniref50\"\n",
    "\n",
    "AA_ALPHABET = [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]\n",
    "STRUCTURE_ALPHABET = [x.lower() for x in AA_ALPHABET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SCOP40_SEQUENCES_FILE, \"r\") as f:\n",
    "    sequences = json.load(f)\n",
    "\n",
    "sequences_with_x = sum(1 for seq in sequences.values() if \"X\" in seq.upper())\n",
    "total_x_chars = sum(seq.upper().count(\"X\") for seq in sequences.values())\n",
    "\n",
    "print(\"Number of sequences: \", len(sequences))\n",
    "print(f\"Number of sequences containing X: {sequences_with_x}\")\n",
    "print(\"Fraction of sequences containing X: \", sequences_with_x / len(sequences))\n",
    "print()\n",
    "print(\"Total number of characters across all sequences: \", sum(len(seq) for seq in sequences.values()))\n",
    "print(f\"Total number of X characters across all sequences: {total_x_chars}\")\n",
    "print(f\"Fraction of X characters across all sequences: {total_x_chars / sum(len(seq) for seq in sequences.values())}\")\n",
    "print()\n",
    "\n",
    "named_sequences = dict(list(sequences.items())[:33])\n",
    "\n",
    "# for i, (k, v) in enumerate(sequences.items()):\n",
    "#     print(f\"{i} {k} {len(v)}: {v}\")\n",
    "# print()\n",
    "\n",
    "for k, v in named_sequences.items():\n",
    "    named_sequences[k] = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", v)))\n",
    "\n",
    "for i, (k, v) in enumerate(named_sequences.items()):\n",
    "    if i == 4:\n",
    "        break\n",
    "    print(f\"{i} {k} {len(v)}: {v}\")\n",
    "print()\n",
    "\n",
    "seq_lengths = {k: len(v.replace(\" \", \"\")) for k, v in named_sequences.items()}\n",
    "longest_seq = max(seq_lengths.items(), key=lambda x: x[1])\n",
    "shortest_seq = min(seq_lengths.items(), key=lambda x: x[1])\n",
    "\n",
    "print(f\"Longest sequence: {longest_seq[0]} with length {longest_seq[1]}\")\n",
    "print(f\"Shortest sequence: {shortest_seq[0]} with length {shortest_seq[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=PROTEIN_ENCODER_NAME,\n",
    "    do_lower_case=False,\n",
    "    use_fast=True,\n",
    "    legacy=False,\n",
    ")\n",
    "\n",
    "model_config = MDPSSMConfig()\n",
    "model = T5EncoderModelForPssmGeneration(model_config)\n",
    "model.load_adapter(MODEL_PATH)\n",
    "model.to(device)\n",
    "\n",
    "# protein_tokens = tokenizer(list(sequences.values()), return_tensors=\"pt\", padding=True, truncation=False)\n",
    "# protein_tokens = {k: v.to(device) for k, v in protein_tokens.items()}\n",
    "\n",
    "# decoded_sequence = tokenizer.decode(protein_tokens[\"input_ids\"][0], skip_special_tokens=False)\n",
    "# print(decoded_sequence)\n",
    "# print(*protein_tokens[\"attention_mask\"][0].tolist())\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     protein_emb = model(\n",
    "#         input_ids=protein_tokens[\"input_ids\"],\n",
    "#         attention_mask=protein_tokens[\"attention_mask\"],\n",
    "#         return_dict=True,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequence = [\" \".join(random.choices(AA_ALPHABET, k=1000)) for x in range(10)]\n",
    "# print(test_sequence)\n",
    "# protein_tokens = tokenizer(test_sequence, return_tensors=\"pt\", padding=True, truncation=False)\n",
    "# protein_tokens.to(device)\n",
    "# print(protein_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pssm(pssm, mask, original_sequence, prost_values=None):\n",
    "    plt.figure(figsize=(30, 7))\n",
    "    sns.heatmap(\n",
    "        pssm.T,\n",
    "        cmap=\"viridis\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cbar_kws={\"label\": \"Probability\"},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"black\",\n",
    "    )\n",
    "    plt.xticks(np.arange(len(original_sequence)) + 0.5, original_sequence, rotation=0, fontfamily=\"monospace\")\n",
    "    print()\n",
    "\n",
    "    plt.yticks(np.arange(len(STRUCTURE_ALPHABET)) + 0.5, STRUCTURE_ALPHABET, rotation=0, fontfamily=\"monospace\")\n",
    "\n",
    "    plt.title(\"PSSM Heatmap\", fontfamily=\"monospace\")\n",
    "    plt.xlabel(\"Sequence (Top: Amino Acid, Bottom: 3Di with ProstT5)\", fontfamily=\"monospace\")\n",
    "    plt.ylabel(\"3Di\", fontfamily=\"monospace\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_batch(model, input_ids, attention_mask):\n",
    "    # print(*attention_mask[0].tolist(), sep=\"\")\n",
    "    # print(*tokenizer.decode(input_ids[0], skip_special_tokens=True).replace(\" \", \"\"), sep=\"\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model_output = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "    for pssm, mask, ids in list(zip(model_output.pssms, model_output.masks, input_ids)):\n",
    "        original_sequence = tokenizer.decode(ids, skip_special_tokens=True).replace(\" \", \"\")\n",
    "        # pssm = pssm.cpu().numpy()\n",
    "        # pssm = pssm[mask.cpu().numpy().astype(bool)]\n",
    "        # print(original_sequence)\n",
    "        # print(*mask.tolist(), sep=\"\")\n",
    "        # print(original_sequence)\n",
    "        # plot_pssm(pssm, mask.cpu().numpy(), original_sequence)\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "named_sequences_items = list(named_sequences.items())\n",
    "for i in range(0, len(named_sequences_items), batch_size):\n",
    "    batch = dict(named_sequences_items[i : i + batch_size])\n",
    "    protein_tokens = tokenizer(list(batch.values()), return_tensors=\"pt\", padding=True, truncation=False)\n",
    "    protein_tokens = {k: v.to(device) for k, v in protein_tokens.items()}\n",
    "    print(protein_tokens)\n",
    "\n",
    "\n",
    "# run_batch(model, protein_tokens[\"input_ids\"], protein_tokens[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_prost = T5Tokenizer.from_pretrained(\"Rostlab/ProstT5\", do_lower_case=False)\n",
    "model_prost = AutoModelForSeq2SeqLM.from_pretrained(\"Rostlab/ProstT5\").to(device)\n",
    "model_prost.float() if device.type == \"cpu\" else model_prost.half()\n",
    "\n",
    "sequence_examples = [\"<AA2fold> \" + s for s in sequences.values()]\n",
    "ids = tokenizer_prost.batch_encode_plus(sequence_examples, add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\").to(\n",
    "    device\n",
    ")\n",
    "\n",
    "index = 0\n",
    "\n",
    "print(len(list(sequences.values())[0]))\n",
    "print(*[f\"{x:<10}\" for x in re.split(\"><| |</\", tokenizer_prost.decode(ids[\"input_ids\"][index]))])\n",
    "\n",
    "print(*[f\"{x:<10}\" for x in ids[\"input_ids\"][index].tolist()])\n",
    "print(*[f\"{x:<10}\" for x in ids[\"attention_mask\"][index].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs_aa2fold = {\n",
    "    \"do_sample\": True,\n",
    "    \"num_beams\": 3,\n",
    "    \"top_p\": 0.95,\n",
    "    \"temperature\": 1.2,\n",
    "    \"top_k\": 6,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prost.eval()\n",
    "with torch.no_grad():\n",
    "    translations = model_prost.generate(\n",
    "        ids.input_ids,\n",
    "        attention_mask=ids.attention_mask,\n",
    "        early_stopping=True,\n",
    "        num_return_sequences=1,\n",
    "        max_length=ids.input_ids.shape[1],\n",
    "        **gen_kwargs_aa2fold,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_translations = tokenizer_prost.batch_decode(translations, skip_special_tokens=True)\n",
    "structure_sequences = [\"\".join(ts.split(\" \")) for ts in decoded_translations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translations)\n",
    "print(structure_sequences)\n",
    "for x, y in zip(structure_sequences, sequences.values()):\n",
    "    print(len(x), len(y.replace(\" \", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "\n",
    "for x in sequences.values():\n",
    "    total += len(x.replace(\" \", \"\"))\n",
    "    print(x.replace(\" \", \"\"))\n",
    "print(total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "for i, ((k, v), (logits, mask)) in enumerate(zip(sequences.items(), zip(protein_emb.pssms, protein_tokens[\"attention_mask\"]))):\n",
    "    # if i == 2:\n",
    "    #     break\n",
    "\n",
    "    df_pssms = pd.DataFrame(logits.cpu().numpy()[: mask.sum()])\n",
    "    original_sequence = v.split()\n",
    "    original_sequence = [f\"{a}\\n{b}\" for a, b in zip(original_sequence, structure_sequences[i])]\n",
    "\n",
    "    # fig = px.imshow(\n",
    "    #     df_pssms.T,\n",
    "    #     color_continuous_scale=\"viridis\",\n",
    "    #     range_color=[0, 1],\n",
    "    #     title=f\"PSSM Heatmap for {k}\",\n",
    "    #     labels={\"x\": \"Sequence\", \"y\": \"Position\"},\n",
    "    # )\n",
    "\n",
    "    # # Create labels combining original sequence and structure sequence\n",
    "    # combined_labels = [f\"{a}\\n{b}\" for a, b in zip(original_sequence, structure_sequences[i])]\n",
    "    # fig.update_xaxes(ticktext=combined_labels, tickvals=list(range(len(original_sequence))))\n",
    "    # fig.show()\n",
    "\n",
    "    plt.figure(figsize=(30, 7))\n",
    "    sns.heatmap(\n",
    "        df_pssms.T,\n",
    "        cmap=\"viridis\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cbar_kws={\"label\": \"Probability\"},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"black\",\n",
    "    )\n",
    "    plt.xticks(np.arange(len(original_sequence)) + 0.5, original_sequence, rotation=0, fontfamily=\"monospace\")\n",
    "    print()\n",
    "\n",
    "    plt.yticks(np.arange(len(STRUCTURE_ALPHABET)) + 0.5, STRUCTURE_ALPHABET, rotation=0, fontfamily=\"monospace\")\n",
    "\n",
    "    plt.title(\"PSSM Heatmap\", fontfamily=\"monospace\")\n",
    "    plt.xlabel(\"Sequence (Top: Amino Acid, Bottom: 3Di with ProstT5)\", fontfamily=\"monospace\")\n",
    "    plt.ylabel(\"3Di\", fontfamily=\"monospace\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
