{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import (\n",
    "    KLDivLoss,\n",
    ")\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    ")\n",
    "\n",
    "from src.model.configuration_md_pssm import MDPSSMConfig\n",
    "from src.model.modeling_outputs import PSSMOutput\n",
    "\n",
    "from plms import ProstT5, PLMConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSSMHead(nn.Module):\n",
    "    \"\"\"Head for PSSM generation from T5 embeddings. based on https://github.com/hefeda/PGP/blob/master/prott5_batch_predictor.py#L144\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (MDPSSMConfig): Configuration object for the model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv1d(1024, 32, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv1d(32, 20, kernel_size=7, padding=3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.classifier(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        pssm = torch.softmax(x, dim=2)\n",
    "        return pssm\n",
    "\n",
    "\n",
    "class T5EncoderModelForPssmGeneration(PreTrainedModel):\n",
    "    def __init__(self, config: MDPSSMConfig):\n",
    "        super().__init__(config=config)\n",
    "        device_map = config.device if hasattr(config, \"device\") else \"auto\"\n",
    "        plm_config = PLMConfig(\n",
    "            name_or_path=config.model_name,\n",
    "            device=device_map,\n",
    "        )\n",
    "\n",
    "        self.protein_encoder = ProstT5(config=plm_config)\n",
    "        self.pssm_head = PSSMHead()\n",
    "        self.loss_fct = KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        encoder_outputs = self.protein_encoder.forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # [batch_size, seq_len, hidden_dim]\n",
    "        hidden_states = encoder_outputs[\"last_hidden_state\"]\n",
    "\n",
    "        # print(attention_mask.shape)\n",
    "        # print(hidden_states.shape)\n",
    "\n",
    "        # display(attention_mask)\n",
    "        # display(hidden_states)\n",
    "\n",
    "        seq_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_indices = torch.arange(attention_mask.size(0), device=attention_mask.device)\n",
    "        attention_mask[batch_indices, seq_lengths] = 0\n",
    "\n",
    "        hidden_states = hidden_states * attention_mask.unsqueeze(-1)\n",
    "\n",
    "        # [batch_size, seq_len, 20]\n",
    "        pssm = self.pssm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # [batch_size * seq_len, 20]\n",
    "            target = labels.flatten(end_dim=1)\n",
    "            pred = pssm.flatten(end_dim=1)\n",
    "\n",
    "            mask = ~torch.any(target == -100, dim=1)\n",
    "\n",
    "            pred = pred[mask]\n",
    "            target = target[mask]\n",
    "\n",
    "            loss = self.loss_fct(torch.log(pred), target)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (pssm, encoder_outputs[2:-1])\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return PSSMOutput(\n",
    "            loss=loss,\n",
    "            pssms=pssm,\n",
    "            hidden_states=encoder_outputs[\"last_hidden_state\"] if output_hidden_states else None,\n",
    "            masks=attention_mask,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "config = MDPSSMConfig(device=device)\n",
    "model = T5EncoderModelForPssmGeneration(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e29b69cec9c42a6ab3d4343fea27788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/11.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5EncoderModel\n",
    "\n",
    "model = T5EncoderModel.from_pretrained(\n",
    "    \"Rostlab/prot_t5_xl_uniref50\",\n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
