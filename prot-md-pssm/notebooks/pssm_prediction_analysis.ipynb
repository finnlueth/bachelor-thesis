{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# for x in dataset[\"input_ids\"]:\n",
    "#     print(tokenizer.decode(x))\n",
    "\n",
    "# unique_sequences = set()\n",
    "# for x in dataset[\"input_ids\"]:\n",
    "#     sequence = tokenizer.decode(x)\n",
    "#     unique_sequences.add(sequence)\n",
    "\n",
    "# print(f\"Number of unique sequences: {len(unique_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tokens = [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]\n",
    "token2idx = dict(zip(tokens, range(len(tokens))))\n",
    "idx2token = dict(zip(range(len(tokens)), tokens))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dataset_reloaded = load_from_disk(\"../tmp/data/pssm/pssm_dataset_0_only\")\n",
    "dataset_reloaded = dataset_reloaded.rename_column(\"pssm_features\", \"labels\")\n",
    "print(dataset_reloaded)\n",
    "\n",
    "\n",
    "def plot_pssm_heatmap(probs, tile_extension=\"\", figsize=(12, 8)):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    if torch.is_tensor(probs):\n",
    "        probs = probs.cpu().numpy()\n",
    "\n",
    "    sns.heatmap(probs, xticklabels=tokens, yticklabels=True, cmap=\"YlOrRd\", vmin=0, vmax=1, cbar_kws={\"label\": \"Probability\"})\n",
    "\n",
    "    plt.xlabel(\"Amino Acids\")\n",
    "    plt.ylabel(\"Sequence Position\")\n",
    "    plt.title(f\"PSSM Heatmap {tile_extension}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    index = 385\n",
    "    for x in range(index, index + 2):\n",
    "        test_input_ids = torch.tensor([dataset_reloaded[x][\"input_ids\"]]).to(device)\n",
    "        test_attention_mask = torch.tensor([dataset_reloaded[x][\"attention_mask\"]]).to(device)\n",
    "\n",
    "        seq_len = (test_attention_mask[0] == 1).sum() - 1\n",
    "        print(f\"Sequence length: {seq_len}\")\n",
    "\n",
    "        outs = model.forward(\n",
    "            input_ids=test_input_ids,\n",
    "            attention_mask=test_attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        pred_probs = outs.logits[0][:seq_len]\n",
    "        target_probs = torch.tensor(dataset_reloaded[x][\"labels\"]).to(device)\n",
    "\n",
    "        # display(pd.DataFrame(pred_probs.cpu()))\n",
    "        plot_pssm_heatmap(pd.DataFrame(pred_probs.cpu()), tile_extension=f\"Predicted {dataset_reloaded[x]['name']}\")\n",
    "\n",
    "        # display(pd.DataFrame(target_probs.cpu()))\n",
    "        plot_pssm_heatmap(pd.DataFrame(target_probs.cpu()), tile_extension=f\"Truth {dataset_reloaded[x]['name']}\")\n",
    "\n",
    "        kl_div_1 = torch.nn.functional.kl_div(torch.log(pred_probs + 1e-10), target_probs, reduction=\"batchmean\")\n",
    "        kl_div_2 = torch.nn.functional.kl_div(torch.log(target_probs + 1e-10), pred_probs, reduction=\"batchmean\")\n",
    "        print(f\"KL Divergence 1: {kl_div_1:.4f}\")\n",
    "        print(f\"KL Divergence 2: {kl_div_2:.4f}\")\n",
    "        print(f\"KL Divergence Total: {kl_div_1 + kl_div_2:.4f}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "model.train()\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
