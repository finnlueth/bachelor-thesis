{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in dataset[\"input_ids\"]:\n",
    "#     print(tokenizer.decode(x))\n",
    "\n",
    "# unique_sequences = set()\n",
    "# for x in dataset[\"input_ids\"]:\n",
    "#     sequence = tokenizer.decode(x)\n",
    "#     unique_sequences.add(sequence)\n",
    "\n",
    "# print(f\"Number of unique sequences: {len(unique_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-kacj50wa because the default path (/home/lfi/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m token2idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(tokens, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens))))\n\u001b[1;32m      6\u001b[0m idx2token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens)), tokens))\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     10\u001b[0m dataset_reloaded \u001b[38;5;241m=\u001b[39m load_from_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../tmp/data/pssm/pssm_dataset_0_only\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m dataset_reloaded \u001b[38;5;241m=\u001b[39m dataset_reloaded\u001b[38;5;241m.\u001b[39mrename_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpssm_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tokens = [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]\n",
    "token2idx = dict(zip(tokens, range(len(tokens))))\n",
    "idx2token = dict(zip(range(len(tokens)), tokens))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dataset_reloaded = load_from_disk(\"../tmp/data/pssm/pssm_dataset_0_only\")\n",
    "dataset_reloaded = dataset_reloaded.rename_column(\"pssm_features\", \"labels\")\n",
    "print(dataset_reloaded)\n",
    "\n",
    "\n",
    "def plot_pssm_heatmap(probs, tile_extension=\"\", figsize=(12, 8)):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    if torch.is_tensor(probs):\n",
    "        probs = probs.cpu().numpy()\n",
    "\n",
    "    sns.heatmap(probs, xticklabels=tokens, yticklabels=True, cmap=\"YlOrRd\", vmin=0, vmax=1, cbar_kws={\"label\": \"Probability\"})\n",
    "\n",
    "    plt.xlabel(\"Amino Acids\")\n",
    "    plt.ylabel(\"Sequence Position\")\n",
    "    plt.title(f\"PSSM Heatmap {tile_extension}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    index = 385\n",
    "    for x in range(index, index + 2):\n",
    "        test_input_ids = torch.tensor([dataset_reloaded[x][\"input_ids\"]]).to(device)\n",
    "        test_attention_mask = torch.tensor([dataset_reloaded[x][\"attention_mask\"]]).to(device)\n",
    "\n",
    "        seq_len = (test_attention_mask[0] == 1).sum() - 1\n",
    "        print(f\"Sequence length: {seq_len}\")\n",
    "\n",
    "        outs = model.forward(\n",
    "            input_ids=test_input_ids,\n",
    "            attention_mask=test_attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        pred_probs = outs.logits[0][:seq_len]\n",
    "        target_probs = torch.tensor(dataset_reloaded[x][\"labels\"]).to(device)\n",
    "\n",
    "        # display(pd.DataFrame(pred_probs.cpu()))\n",
    "        plot_pssm_heatmap(pd.DataFrame(pred_probs.cpu()), tile_extension=f\"Predicted {dataset_reloaded[x]['name']}\")\n",
    "\n",
    "        # display(pd.DataFrame(target_probs.cpu()))\n",
    "        plot_pssm_heatmap(pd.DataFrame(target_probs.cpu()), tile_extension=f\"Truth {dataset_reloaded[x]['name']}\")\n",
    "\n",
    "        kl_div_1 = torch.nn.functional.kl_div(torch.log(pred_probs + 1e-10), target_probs, reduction=\"batchmean\")\n",
    "        kl_div_2 = torch.nn.functional.kl_div(torch.log(target_probs + 1e-10), pred_probs, reduction=\"batchmean\")\n",
    "        print(f\"KL Divergence 1: {kl_div_1:.4f}\")\n",
    "        print(f\"KL Divergence 2: {kl_div_2:.4f}\")\n",
    "        print(f\"KL Divergence Total: {kl_div_1 + kl_div_2:.4f}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "model.train()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
