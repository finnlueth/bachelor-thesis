{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import socket\n",
    "import urllib.request\n",
    "import sys\n",
    "\n",
    "\n",
    "os.environ[\"NO_PROXY\"] = \"*\"\n",
    "os.environ[\"no_proxy\"] = \"*\"\n",
    "\n",
    "\n",
    "def no_network(*args, **kwargs):\n",
    "    raise Exception(\"Network access disabled\")\n",
    "\n",
    "\n",
    "socket.socket = no_network\n",
    "urllib.request.urlopen = no_network\n",
    "\n",
    "sys.path.append(\"./bio2token_main\")\n",
    "\n",
    "from typing import *\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from bio2token_main.bio2token.models.fsq_ae import FSQ_AE\n",
    "from bio2token_main.bio2token.utils.utils import *\n",
    "from bio2token_main.bio2token.utils.pdb import *\n",
    "\n",
    "import warnings\n",
    "import Bio\n",
    "\n",
    "warnings.simplefilter(\"ignore\", Bio.PDB.PDBExceptions.PDBConstructionWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = \"./bio2token_main/\"\n",
    "chains = None\n",
    "pdb = \"3wbm\"\n",
    "seq_type = \"AA\"\n",
    "tokenizer = \"bio2token\"\n",
    "\n",
    "config_model = load_config(TOKENIZER_PATH + \"configs/tokenizer.yaml\")\n",
    "model = FSQ_AE(config_model).cuda()\n",
    "\n",
    "checkpoint = TOKENIZER_PATH + \"checkpoints/bio2token.ckpt\"\n",
    "state_dict = torch.load(checkpoint, map_location=\"cuda:0\")[\"state_dict\"]\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    new_state_dict[k.replace(\"model.\", \"\")] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASP Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR = os.path.join(\"./data/casp15_backbone_only/\")\n",
    "OUT_DIR = os.path.join(\"./data/bio2token/bio2token_casp_backbone_recon\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "file_names = os.listdir(IN_DIR)\n",
    "file_names.sort()\n",
    "\n",
    "for file_name in file_names:\n",
    "    print(f\"Processing {file_name} from {IN_DIR}\")\n",
    "    biomolecule = pdb_2_dict(\n",
    "        os.path.join(IN_DIR, file_name),\n",
    "        chains=chains,\n",
    "    )\n",
    "    batch = pdb_to_batch(config_model, biomolecule)\n",
    "    batch[\"seq_type\"] = seq_type\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.step(batch, mode=\"inference\")\n",
    "\n",
    "    gt = out[\"coords_gt\"][:, : biomolecule[\"atom_length\"], :].squeeze(0).cpu().numpy()\n",
    "    gt = np.split(gt, gt.shape[0])\n",
    "    recon = (\n",
    "        out[\"coords_pred_kabsch_all\"][:, : biomolecule[\"atom_length\"], :]\n",
    "        .squeeze(0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    recon = np.split(recon, recon.shape[0])\n",
    "\n",
    "    # pdb_dict_gt = to_pdb_dict(\n",
    "    #     coords=gt,\n",
    "    #     atom_names=biomolecule[\"atom_names\"],\n",
    "    #     continuous_res_ids=biomolecule[\"continuous_res_ids\"],\n",
    "    #     residue_names=biomolecule[\"res_names\"],\n",
    "    #     res_types=biomolecule[\"res_types\"],\n",
    "    # )\n",
    "    pdb_dict_recon = to_pdb_dict(\n",
    "        coords=recon,\n",
    "        atom_names=biomolecule[\"atom_names\"],\n",
    "        continuous_res_ids=biomolecule[\"continuous_res_ids\"],\n",
    "        residue_names=biomolecule[\"res_names\"],\n",
    "        res_types=biomolecule[\"res_types\"],\n",
    "    )\n",
    "\n",
    "    count_atoms = 0\n",
    "    for k, v in pdb_dict_recon.items():\n",
    "        count_atoms += len(v[\"atom_names\"])\n",
    "\n",
    "    # pdb_dict_to_file(\n",
    "    #     pdb_dict_gt,\n",
    "    #     pdb_file_path=os.path.join(\n",
    "    #         \"./data/bio2token/examples/recon\", f\"{pdb}_{tokenizer}_gt.pdb\"\n",
    "    #     ),\n",
    "    # )\n",
    "    output_file = os.path.join(OUT_DIR, f\"{os.path.splitext(file_name)[0]}.pdb\")\n",
    "    pdb_dict_to_file(\n",
    "        pdb_dict_recon,\n",
    "        pdb_file_path=output_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATH 4.2.0 Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR = os.path.join(\"./data/cath-dataset-nonredundant-S40-v4_2_0.pdb/\")\n",
    "OUT_DIR = os.path.join(\"./data/cath_4_2_0_bio2token\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "file_names = os.listdir(IN_DIR)\n",
    "file_names.sort()\n",
    "\n",
    "for file_name in file_names:\n",
    "    print(f\"Processing {file_name} from {IN_DIR}\")\n",
    "    biomolecule = pdb_2_dict(\n",
    "        os.path.join(IN_DIR, file_name),\n",
    "        chains=chains,\n",
    "    )\n",
    "    batch = pdb_to_batch(config_model, biomolecule)\n",
    "    batch[\"seq_type\"] = seq_type\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.step(batch, mode=\"inference\")\n",
    "\n",
    "    gt = out[\"coords_gt\"][:, : biomolecule[\"atom_length\"], :].squeeze(0).cpu().numpy()\n",
    "    gt = np.split(gt, gt.shape[0])\n",
    "    recon = (\n",
    "        out[\"coords_pred_kabsch_all\"][:, : biomolecule[\"atom_length\"], :]\n",
    "        .squeeze(0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    recon = np.split(recon, recon.shape[0])\n",
    "\n",
    "    # pdb_dict_gt = to_pdb_dict(\n",
    "    #     coords=gt,\n",
    "    #     atom_names=biomolecule[\"atom_names\"],\n",
    "    #     continuous_res_ids=biomolecule[\"continuous_res_ids\"],\n",
    "    #     residue_names=biomolecule[\"res_names\"],\n",
    "    #     res_types=biomolecule[\"res_types\"],\n",
    "    # )\n",
    "    pdb_dict_recon = to_pdb_dict(\n",
    "        coords=recon,\n",
    "        atom_names=biomolecule[\"atom_names\"],\n",
    "        continuous_res_ids=biomolecule[\"continuous_res_ids\"],\n",
    "        residue_names=biomolecule[\"res_names\"],\n",
    "        res_types=biomolecule[\"res_types\"],\n",
    "    )\n",
    "\n",
    "    count_atoms = 0\n",
    "    for k, v in pdb_dict_recon.items():\n",
    "        count_atoms += len(v[\"atom_names\"])\n",
    "\n",
    "    # pdb_dict_to_file(\n",
    "    #     pdb_dict_gt,\n",
    "    #     pdb_file_path=os.path.join(\n",
    "    #         \"./data/bio2token/examples/recon\", f\"{pdb}_{tokenizer}_gt.pdb\"\n",
    "    #     ),\n",
    "    # )\n",
    "    output_file = os.path.join(OUT_DIR, f\"{os.path.splitext(file_name)[0]}.pdb\")\n",
    "    pdb_dict_to_file(\n",
    "        pdb_dict_recon,\n",
    "        pdb_file_path=output_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
