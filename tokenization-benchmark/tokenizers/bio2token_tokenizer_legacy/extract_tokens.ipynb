{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import socket\n",
    "import urllib.request\n",
    "import sys\n",
    "\n",
    "\n",
    "os.environ[\"NO_PROXY\"] = \"*\"\n",
    "os.environ[\"no_proxy\"] = \"*\"\n",
    "\n",
    "\n",
    "def no_network(*args, **kwargs):\n",
    "    raise Exception(\"Network access disabled\")\n",
    "\n",
    "\n",
    "socket.socket = no_network\n",
    "urllib.request.urlopen = no_network\n",
    "\n",
    "sys.path.append(\"./bio2token_main\")\n",
    "\n",
    "from typing import *\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from bio2token_main.bio2token.models.fsq_ae import FSQ_AE\n",
    "from bio2token_main.bio2token.utils.utils import load_config\n",
    "from bio2token_main.bio2token.utils.pdb import (\n",
    "    pdb_2_dict,\n",
    "    pdb_to_batch,\n",
    "    pdb_dict_to_file,\n",
    "    to_pdb_dict,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "import Bio\n",
    "\n",
    "warnings.simplefilter(\"ignore\", Bio.PDB.PDBExceptions.PDBConstructionWarning)\n",
    "\n",
    "\n",
    "from bio2token_extension import Bio2TokenExtension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bio2TokenExtension(\n",
       "  (encoder): MambaLMHeadModel(\n",
       "    (backbone): MixerModel(\n",
       "      (embedding): Linear(in_features=3, out_features=128, bias=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Block(\n",
       "          (mixer): Mamba(\n",
       "            (in_proj): Linear(in_features=128, out_features=512, bias=False)\n",
       "            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=256, out_features=40, bias=False)\n",
       "            (dt_proj): Linear(in_features=8, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=128, bias=False)\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm_f): RMSNorm()\n",
       "    )\n",
       "    (lm_head): Identity()\n",
       "  )\n",
       "  (decoder): MambaLMHeadModel(\n",
       "    (backbone): MixerModel(\n",
       "      (embedding): Identity()\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x Block(\n",
       "          (mixer): Mamba(\n",
       "            (in_proj): Linear(in_features=128, out_features=512, bias=False)\n",
       "            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=256, out_features=40, bias=False)\n",
       "            (dt_proj): Linear(in_features=8, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=128, bias=False)\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm_f): RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=128, out_features=3, bias=False)\n",
       "  )\n",
       "  (fsq): FSQ(\n",
       "    (project_in): Linear(in_features=128, out_features=6, bias=True)\n",
       "    (project_out): Linear(in_features=6, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER_PATH = \"./bio2token_main/\"\n",
    "chains = None\n",
    "pdb = \"3wbm\"\n",
    "seq_type = \"AA\"\n",
    "tokenizer = \"bio2token\"\n",
    "\n",
    "config_model = load_config(TOKENIZER_PATH + \"configs/tokenizer.yaml\")\n",
    "model = Bio2TokenExtension(config_model).cuda()\n",
    "\n",
    "checkpoint = TOKENIZER_PATH + \"checkpoints/bio2token.ckpt\"\n",
    "state_dict = torch.load(checkpoint, map_location=\"cuda:0\")[\"state_dict\"]\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    new_state_dict[k.replace(\"model.\", \"\")] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1\n"
     ]
    }
   ],
   "source": [
    "IN_DIR = os.path.join(\"./data/cath-dataset-nonredundant-S40-v4_2_0\")\n",
    "OUT_DIR = os.path.join(\"./data/bio2token_tokens/cath_4_2_0_bio2token\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "file_names = os.listdir(IN_DIR)[:1]\n",
    "file_names.sort()\n",
    "\n",
    "print(\"Number of files:\", len(file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bio2token_extension.Bio2TokenExtension"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_names:\n",
    "    biomolecule = pdb_2_dict(\n",
    "        pdb_path=os.path.join(IN_DIR, file_name),\n",
    "        chains=chains,\n",
    "    )\n",
    "    batch = pdb_to_batch(config_model, biomolecule)\n",
    "    batch[\"seq_type\"] = seq_type\n",
    "    with torch.no_grad():\n",
    "        out = model.step(batch, mode=\"inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pdb_id', 'seq', 'res_names', 'coords_groundtruth', 'atom_names', 'atom_types', 'seq_length', 'atom_length', 'chains', 'res_ids', 'continuous_res_ids', 'res_types'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biomolecule.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.339, -14.972,   8.619],\n",
       "       [ -2.357, -15.875,   8.061],\n",
       "       [ -3.404, -15.142,   7.224],\n",
       "       ...,\n",
       "       [ -0.381,   4.085,   1.121],\n",
       "       [  1.014,   3.971,   0.789],\n",
       "       [ -1.1  ,   3.13 ,   0.148]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biomolecule[\"coords_groundtruth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 135301, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"coords_pred_kabsch_all\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_names:\n",
    "    # print(f\"Processing {file_name} from {IN_DIR}\")\n",
    "    biomolecule = pdb_2_dict(\n",
    "        os.path.join(IN_DIR, file_name),\n",
    "        chains=chains,\n",
    "    )\n",
    "    batch = pdb_to_batch(config_model, biomolecule)\n",
    "    batch[\"seq_type\"] = seq_type\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.step(batch, mode=\"inference\")\n",
    "\n",
    "    print(out)\n",
    "\n",
    "    gt = out[\"coords_gt\"][:, : biomolecule[\"atom_length\"], :].squeeze(0).cpu().numpy()\n",
    "    gt = np.split(gt, gt.shape[0])\n",
    "    recon = (\n",
    "        out[\"coords_pred_kabsch_all\"][:, : biomolecule[\"atom_length\"], :]\n",
    "        .squeeze(0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    recon = np.split(recon, recon.shape[0])\n",
    "\n",
    "    # pdb_dict_gt = to_pdb_dict(\n",
    "    #     coords=gt,\n",
    "    #     atom_names=biomolecule[\"atom_names\"],\n",
    "    #     continuous_res_ids=biomolecule[\"continuous_res_ids\"],\n",
    "    #     residue_names=biomolecule[\"res_names\"],\n",
    "    #     res_types=biomolecule[\"res_types\"],\n",
    "    # )\n",
    "    pdb_dict_recon = to_pdb_dict(\n",
    "        coords=recon,\n",
    "        atom_names=biomolecule[\"atom_names\"],\n",
    "        continuous_res_ids=biomolecule[\"continuous_res_ids\"],\n",
    "        residue_names=biomolecule[\"res_names\"],\n",
    "        res_types=biomolecule[\"res_types\"],\n",
    "    )\n",
    "\n",
    "    count_atoms = 0\n",
    "    for k, v in pdb_dict_recon.items():\n",
    "        count_atoms += len(v[\"atom_names\"])\n",
    "\n",
    "    # pdb_dict_to_file(\n",
    "    #     pdb_dict_gt,\n",
    "    #     pdb_file_path=os.path.join(\n",
    "    #         \"./data/bio2token/examples/recon\", f\"{pdb}_{tokenizer}_gt.pdb\"\n",
    "    #     ),\n",
    "    # )\n",
    "    output_file = os.path.join(OUT_DIR, f\"{os.path.splitext(file_name)[0]}.pdb\")\n",
    "    pdb_dict_to_file(\n",
    "        pdb_dict_recon,\n",
    "        pdb_file_path=output_file,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
